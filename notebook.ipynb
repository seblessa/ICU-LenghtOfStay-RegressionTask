{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "import utils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, count\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType, IntegerType, DoubleType\n",
    "\n",
    "label_column = 'LOS'"
   ],
   "id": "500926c2d111d3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Intensive Care Unit Data Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# check spark configs to only errors:\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"1000\")"
   ],
   "id": "16b2dd3593ae37fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the datasets\n",
    "df_admissions = spark.read.csv(\"datasets/ADMISSIONS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_diagnoses = spark.read.csv(\"datasets/DIAGNOSES_ICD.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_icustays = spark.read.csv(\"datasets/ICUSTAYS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_patients = spark.read.csv(\"datasets/PATIENTS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "# df_chartevents = spark.read.csv(\"datasets/CHARTEVENTS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")"
   ],
   "id": "f10eb3861e1a6ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_patients.join(df_admissions, df_patients[\"SUBJECT_ID\"] == df_admissions[\"SUBJECT_ID\"], how=\"left\").drop(df_admissions[\"SUBJECT_ID\"])\n",
    "df = df.join(df_icustays, df[\"HADM_ID\"] == df_icustays[\"HADM_ID\"], how=\"left\").drop(df_icustays[\"SUBJECT_ID\"]).drop(df_icustays[\"HADM_ID\"])\n",
    "df = df.join(df_diagnoses, df[\"SUBJECT_ID\"] == df_diagnoses[\"SUBJECT_ID\"], how=\"left\").drop(df_diagnoses[\"SUBJECT_ID\"]).drop(df_diagnoses[\"HADM_ID\"])"
   ],
   "id": "de5ba1599a4770a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "6f4a65a67efe1407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values in the dataset.",
   "id": "74cdea29e7496f79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "13dcfe5ad8d78111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "id": "10987a3efe7f09bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's create a column called 'AGE'",
   "id": "b13d9d8304f7de2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure 'DOB' and 'ADMITTIME' are in the correct date format if not already\n",
    "df = df.withColumn('DOB', F.to_date('DOB'))\n",
    "df = df.withColumn('ADMITTIME', F.to_date('ADMITTIME'))\n",
    "\n",
    "# Create the 'AGE' column by calculating the difference in years between 'ADMITTIME' and 'DOB'\n",
    "df = df.withColumn('AGE', F.expr(\"floor(months_between(ADMITTIME, DOB) / 12)\"))"
   ],
   "id": "def300029631d207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It only matters if the patient is dead or not, not the date of death, so we can drop the date of death columns. And we also drop other death-related columns.",
   "id": "9823fde2d264c156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df=df.drop(\"DOB\").drop(\"DOD\").drop(\"DOD_SSN\").drop(\"EXPIRE_FLAG\").drop(\"DEATHTIME\")\n",
    "df = df.withColumn(\"DOD_HOSP\", F.when(F.col(\"DOD_HOSP\").isNull(), 0).otherwise(1))\n",
    "df = df.withColumnRenamed(\"DOD_HOSP\", \"DIED\")\n",
    "df = df.withColumnRenamed(\"ICD9_CODE\",\"DISEASES_CODE\")"
   ],
   "id": "2bde8175746ee315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we will drop the columns that are not useful for the analysis.",
   "id": "41180090f1dbf0a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_remove = [\n",
    "    \"ADMITTIME\", \"DISCHTIME\", \"EDREGTIME\", \"EDOUTTIME\", \"HOSPITAL_EXPIRE_FLAG\",\n",
    "    \"INTIME\", \"OUTTIME\",\"LANGUAGE\",\"DISCHARGE_LOCATION\",\n",
    "    \"ICUSTAY_ID\", \"SEQ_NUM\",\"HAS_CHARTEVENTS_DATA\",\"DBSOURCE\"\n",
    "]\n",
    "\n",
    "df = df.drop(*columns_to_remove)"
   ],
   "id": "d245622e4fc0f44b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The ethnicity column has too many unique values. We can group them into a few categories.",
   "id": "7776beac1e92e389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the transformation using multiple chained when conditions\n",
    "df = df.withColumn(\"ETHNICITY\",\n",
    "    F.when(F.col(\"ETHNICITY\").isin('AMERICAN INDIAN/ALASKA NATIVE', 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE'),\n",
    "           'American Indian/Alaska Native')\n",
    "    .when(F.col(\"ETHNICITY\").isin('ASIAN', 'ASIAN - ASIAN INDIAN', 'ASIAN - CAMBODIAN', 'ASIAN - CHINESE', 'ASIAN - FILIPINO', 'ASIAN - JAPANESE', 'ASIAN - KOREAN', 'ASIAN - OTHER', 'ASIAN - THAI', 'ASIAN - VIETNAMESE'),\n",
    "          'Asian')\n",
    "    .when(F.col(\"ETHNICITY\").isin('BLACK/AFRICAN', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN', 'BLACK/HAITIAN'),\n",
    "          'Black')\n",
    "    .when(F.col(\"ETHNICITY\").isin('HISPANIC OR LATINO', 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)', 'HISPANIC/LATINO - COLOMBIAN', 'HISPANIC/LATINO - CUBAN', 'HISPANIC/LATINO - DOMINICAN', 'HISPANIC/LATINO - GUATEMALAN', 'HISPANIC/LATINO - HONDURAN', 'HISPANIC/LATINO - MEXICAN', 'HISPANIC/LATINO - PUERTO RICAN', 'HISPANIC/LATINO - SALVADORAN'),\n",
    "          'Hispanic/Latino')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MIDDLE EASTERN'),\n",
    "          'Middle Eastern')\n",
    "    .when(F.col(\"ETHNICITY\").isin('NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER'),\n",
    "          'Pacific Islander')\n",
    "    .when(F.col(\"ETHNICITY\").isin('WHITE', 'WHITE - BRAZILIAN', 'WHITE - EASTERN EUROPEAN', 'WHITE - OTHER EUROPEAN', 'WHITE - RUSSIAN', 'PORTUGUESE'),\n",
    "          'White')\n",
    "    .when(F.col(\"ETHNICITY\").isin('CARIBBEAN ISLAND', 'SOUTH AMERICAN'),\n",
    "          'Caribbean/South American')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MULTI RACE ETHNICITY'),\n",
    "          'Multi-Race')\n",
    "    .when(F.col(\"ETHNICITY\").isin('OTHER'),\n",
    "          'Other')\n",
    "    .otherwise('NO DATA REGISTERED')\n",
    ")"
   ],
   "id": "dab8818e9e64a178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can aggregate the data by primary keys and collect the remaining columns into lists.",
   "id": "dd9393f9b525a73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your primary key columns\n",
    "primary_key_columns = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "# Identify the remaining columns to be grouped\n",
    "remaining_columns = [col for col in df.columns if col not in primary_key_columns]\n",
    "\n",
    "# Group by the primary key columns and aggregate the remaining columns into lists\n",
    "df = df.groupBy(primary_key_columns).agg(*(F.collect_list(col).alias(col) for col in remaining_columns))\n",
    "\n",
    "df=df.drop(\"HADM_ID\")"
   ],
   "id": "426159d6db44f48a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can transform the columns that contain lists of values. If all values in the list are the same, we can replace the list with a single value. If the list is empty, we can replace it with a default value.",
   "id": "85db3f5cb0247fc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "1250530cd4aa7223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "replace_empty_list_udf = udf(utils.replace_empty_list, ArrayType(StringType()))\n",
    "transform_list_udf = udf(utils.transform_list, ArrayType(StringType()))\n",
    "handle_list_udf = udf(utils.handle_list, StringType())\n",
    "empty_list_udf = F.udf(lambda col: col == [], BooleanType())\n",
    "\n",
    "for column in df.columns:    \n",
    "    if isinstance(df.schema[column].dataType, ArrayType):\n",
    "        if df.filter(empty_list_udf(F.col(column))).count() > 0:\n",
    "            df = df.withColumn(column, replace_empty_list_udf(col(column)))\n",
    "            \n",
    "        df = df.withColumn(column, transform_list_udf(col(column)))\n",
    "        if len(set(df.select(column).first()[0])) == 1:\n",
    "            df = df.withColumn(column, handle_list_udf(df[column]))"
   ],
   "id": "caae3630b4ca832b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values again.",
   "id": "f32cfe22255a064f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "93db242a248ae66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "89c7ee6e59b54f6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graphical Analysis",
   "id": "b44f05a9d055d4ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming `df` is your PySpark DataFrame\n",
    "df = df.withColumn(\"DIED\", df[\"DIED\"].cast(\"integer\"))\n",
    "\n",
    "# Plotting death counts by gender\n",
    "utils.plot_graph(df, 'GENDER', 'DIED', F.sum, 'Death Counts by Gender', 'Gender', 'Number of Deaths')"
   ],
   "id": "a566c35651c163bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of marital status",
   "id": "71dc1b8d55dda66e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'MARITAL_STATUS', 'MARITAL_STATUS', F.count, 'Count of Marital Status', 'Marital Status', 'Count')",
   "id": "b649d93d5d41777d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of Admission Types",
   "id": "633859d36b485929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'ADMISSION_TYPE', 'ADMISSION_TYPE', F.count, 'Count of Admission Types', 'Admission Type', 'Count')",
   "id": "8fe6b04f23c180e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Distribution of Lethality by Religion\n",
    "\n",
    "The 1ยบ and 2ยบ deadliest religions are not shown in the graph because they are outliers."
   ],
   "id": "1a2938601acce3b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate death rate per religion\n",
    "death_rate_per_religion = df.groupBy('RELIGION')\\\n",
    "    .agg(\n",
    "        F.sum('DIED').alias('Deaths'), \n",
    "        F.count('DIED').alias('Total')\n",
    "    )\\\n",
    "    .withColumn('Death Rate', (F.col('Deaths') / F.col('Total')) * 100)\\\n",
    "    .orderBy(F.col('Death Rate').desc())\n",
    "\n",
    "religions = death_rate_per_religion.rdd.zipWithIndex().filter(lambda x: x[1] in [2,3,4,5,6,7]).map(lambda x: x[0]).toDF()\n",
    "\n",
    "utils.plot_graph(religions, 'RELIGION', 'Death Rate', F.first, 'Death Rates for Religions Ranked', 'Religion', 'Death Rate (%)')"
   ],
   "id": "f129bac8182bc233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Lethality by Ethnicity",
   "id": "7152930eda22b113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_df = df.filter(df['ETHNICITY'] != 'NO DATA REGISTERED')\n",
    "\n",
    "# Calculate death rate per ethnicity\n",
    "death_rate_per_ethnicity = filtered_df.groupBy('ETHNICITY')\\\n",
    "    .agg(\n",
    "        F.sum('DIED').alias('Deaths'), \n",
    "        F.count('DIED').alias('Total')\n",
    "    )\\\n",
    "    .withColumn('Death Rate', (F.col('Deaths') / F.col('Total')) * 100)\\\n",
    "    .orderBy(F.col('Death Rate').desc())\\\n",
    "    .limit(5)\n",
    "\n",
    "utils.plot_graph(death_rate_per_ethnicity, 'ETHNICITY', 'Death Rate', F.first, 'Top 5 Deadliest Ethnicities by Death Rate', 'Ethnicity', 'Death Rate (%)')"
   ],
   "id": "d96f32c1dc93cd60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Age",
   "id": "bfcc45416965fed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be777315dd7a0653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre processing",
   "id": "52d7e3f536e8fcfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "7f6a614a3202cc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.withColumn(\"GENDER\", F.when(col(\"GENDER\") == \"M\", 0).otherwise(1))\n",
    "check_presence_udf = udf(lambda category_list, value: 1 if value in category_list else 0, IntegerType())\n",
    "\n",
    "# Find unique categories and generate columns for each category\n",
    "for category in df.select(\"DISEASES_CODE\").rdd.flatMap(lambda x: x[0]).distinct().collect():\n",
    "    df = df.withColumn(f'DISEASE_CODE_{category}', check_presence_udf(col(\"DISEASES_CODE\"), F.lit(category)))\n",
    "    \n",
    "df = df.drop(\"DISEASES_CODE\")"
   ],
   "id": "7e8436aa3d732d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_columns = df.columns\n",
    "feature_columns = [col for col in all_columns if col not in ['SUBJECT_ID', 'LOS']]\n",
    "\n",
    "# Manually specified categorical and numerical features\n",
    "categorical_features = ['ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE', 'RELIGION',\n",
    "                        'MARITAL_STATUS', 'ETHNICITY', 'DIAGNOSIS', 'FIRST_CAREUNIT', 'LAST_CAREUNIT']\n",
    "numerical_features = [col for col in feature_columns if col not in categorical_features and col not in ['SUBJECT_ID', 'LOS']]\n",
    "\n",
    "# Cast string numerical features to float\n",
    "df = df.withColumn(\"AGE\", col(\"AGE\").cast(\"float\"))\n",
    "df = df.withColumn(\"FIRST_WARDID\", col(\"FIRST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LAST_WARDID\", col(\"LAST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LOS\", col(\"LOS\").cast(\"float\"))"
   ],
   "id": "dde019ceeaa3b584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "e3f51af334857d5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stages in the pipeline\n",
    "stages = []\n",
    "\n",
    "# Indexing and encoding categorical features\n",
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"ClassVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assemble all the features along with the encoded categorical features\n",
    "assemblerInputs = [c + \"ClassVec\" for c in categorical_features] + numerical_features\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Pipeline: This will ensure all stages are applied in sequence\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_transformed = pipelineModel.transform(df)"
   ],
   "id": "4fc81fbf35516275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume subject_ids have been collected as before\n",
    "subject_ids = [row['SUBJECT_ID'] for row in df_transformed.select(\"SUBJECT_ID\").distinct().collect()]\n",
    "split_index = int(len(subject_ids) * 0.8)\n",
    "\n",
    "train_ids = set(subject_ids[:split_index])\n",
    "test_ids = set(subject_ids[split_index:])\n",
    "\n",
    "# Directly filter the DataFrame using the list\n",
    "train_data = df_transformed.filter(col(\"SUBJECT_ID\").isin(train_ids))\n",
    "test_data = df_transformed.filter(col(\"SUBJECT_ID\").isin(test_ids))\n",
    "\n",
    "# Check the count of entries in each dataset\n",
    "print(\"Entries in train_data:\", train_data.count())\n",
    "print(\"Entries in test_data:\", test_data.count())"
   ],
   "id": "d2ef21a5b75c8ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data.show()",
   "id": "a0cf109ef3698ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_data.show()",
   "id": "6cf58d8ae720ad0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we are making sure that there isn't simultaneously a patient in the test set and train set.",
   "id": "dc2264ddb657c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction",
   "id": "8f4a8adc7d56a64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(predictions, label_column):\n",
    "    # Calculate the Mean Absolute Error (MAE)\n",
    "    mae = predictions.withColumn('abs_error', abs(col(label_column) - col(\"prediction\"))) \\\n",
    "                     .select(F.avg('abs_error')).first()[0]\n",
    "\n",
    "    # Convert MAE to a pseudo-accuracy metric (assuming MAE is less than 100)\n",
    "    accuracy = 100 - mae  # This assumes that lower error corresponds to higher accuracy\n",
    "\n",
    "    return accuracy"
   ],
   "id": "9d1ba85230ad86dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression",
   "id": "ea9c5b48e71b17cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param = {\n",
    "    'maxIter': 10,\n",
    "    'regParam': 0.3,\n",
    "    'elasticNetParam': 0.8\n",
    "}\n",
    "\n",
    "# Define and fit the Linear Regression model on the training set\n",
    "lr = LinearRegression(featuresCol='features', labelCol=label_column, **param)\n",
    "lr_model = lr.fit(train_data)"
   ],
   "id": "ff9b5170b8210b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest Regression",
   "id": "d32a48b1e9035b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param = {\n",
    "    'maxDepth': 5,\n",
    "    'maxBins': 32,\n",
    "    'minInstancesPerNode': 1,\n",
    "    'minInfoGain': 0.0,\n",
    "    'maxMemoryInMB': 256,\n",
    "    'cacheNodeIds': False,\n",
    "    'checkpointInterval': 10,\n",
    "    'impurity': 'variance',\n",
    "    'featureSubsetStrategy': 'auto',\n",
    "    'subsamplingRate': 1.0,\n",
    "    'seed': None,\n",
    "    'numTrees': 20,\n",
    "}\n",
    "\n",
    "# Define and fit the Random Forest model on the training set\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol=label_column, **param)\n",
    "rf_model = rf.fit(train_data)"
   ],
   "id": "31943b611838f331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "7d886ff6fd7e57fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate models using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=label_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Generate predictions on both training and testing datasets\n",
    "lr_train_predictions = lr_model.transform(train_data)\n",
    "lr_test_predictions = lr_model.transform(test_data)\n",
    "rf_train_predictions = rf_model.transform(train_data)\n",
    "rf_test_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Calculate RMSE for Linear Regression\n",
    "lr_train_rmse = evaluator.evaluate(lr_train_predictions)\n",
    "lr_test_rmse = evaluator.evaluate(lr_test_predictions)\n",
    "\n",
    "# Calculate RMSE for Random Forest\n",
    "rf_train_rmse = evaluator.evaluate(rf_train_predictions)\n",
    "rf_test_rmse = evaluator.evaluate(rf_test_predictions)\n",
    "\n",
    "# Print RMSE results\n",
    "print(f\"Linear Regression - Train RMSE: {lr_train_rmse}, Test RMSE: {lr_test_rmse}\")\n",
    "print(f\"Random Forest - Train RMSE: {rf_train_rmse}, Test RMSE: {rf_test_rmse}\")"
   ],
   "id": "5ce0bc3499010522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5c6a816f2d4e6af5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
