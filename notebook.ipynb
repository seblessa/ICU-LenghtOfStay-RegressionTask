{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "LABEL = 'LOS'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Intensive Care Unit Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ],
   "id": "500926c2d111d3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the datasets\n",
    "df_admissions = spark.read.csv(\"datasets/ADMISSIONS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_diagnoses = spark.read.csv(\"datasets/DIAGNOSES_ICD.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_icustays = spark.read.csv(\"datasets/ICUSTAYS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_patients = spark.read.csv(\"datasets/PATIENTS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")"
   ],
   "id": "f10eb3861e1a6ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_patients.join(df_admissions, df_patients[\"SUBJECT_ID\"] == df_admissions[\"SUBJECT_ID\"], how=\"left\").drop(df_admissions[\"SUBJECT_ID\"])\n",
    "df = df.join(df_icustays, df[\"HADM_ID\"] == df_icustays[\"HADM_ID\"], how=\"left\").drop(df_icustays[\"SUBJECT_ID\"]).drop(df_icustays[\"HADM_ID\"])\n",
    "df = df.join(df_diagnoses, df[\"SUBJECT_ID\"] == df_diagnoses[\"SUBJECT_ID\"], how=\"left\").drop(df_diagnoses[\"SUBJECT_ID\"]).drop(df_diagnoses[\"HADM_ID\"])"
   ],
   "id": "de5ba1599a4770a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "6f4a65a67efe1407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values in the dataset.",
   "id": "74cdea29e7496f79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "13dcfe5ad8d78111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "id": "10987a3efe7f09bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's create a column called 'AGE'",
   "id": "b13d9d8304f7de2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure 'DOB' and 'ADMITTIME' are in the correct date format if not already\n",
    "df = df.withColumn('DOB', F.to_date('DOB'))\n",
    "df = df.withColumn('ADMITTIME', F.to_date('ADMITTIME'))\n",
    "\n",
    "# Create the 'AGE' column by calculating the difference in years between 'ADMITTIME' and 'DOB'\n",
    "df = df.withColumn('AGE', F.expr(\"floor(months_between(ADMITTIME, DOB) / 12)\"))"
   ],
   "id": "def300029631d207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It only matters if the patient is dead or not, not the date of death, so we can drop the date of death columns. And we also drop other death-related columns.",
   "id": "9823fde2d264c156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df=df.drop(\"DOB\").drop(\"DOD\").drop(\"DOD_SSN\").drop(\"EXPIRE_FLAG\").drop(\"DEATHTIME\")\n",
    "df = df.withColumn(\"DOD_HOSP\", F.when(F.col(\"DOD_HOSP\").isNull(), 0).otherwise(1))\n",
    "df = df.withColumnRenamed(\"DOD_HOSP\", \"DIED\")\n",
    "df = df.withColumnRenamed(\"ICD9_CODE\",\"DISEASES_CODE\")"
   ],
   "id": "2bde8175746ee315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we will drop the columns that are not useful for the analysis.",
   "id": "41180090f1dbf0a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_remove = [\n",
    "    \"ADMITTIME\", \"DISCHTIME\", \"EDREGTIME\", \"EDOUTTIME\", \"HOSPITAL_EXPIRE_FLAG\",\n",
    "    \"INTIME\", \"OUTTIME\",\"LANGUAGE\",\"DISCHARGE_LOCATION\",\n",
    "    \"ICUSTAY_ID\", \"SEQ_NUM\",\"HAS_CHARTEVENTS_DATA\",\"DBSOURCE\"\n",
    "]\n",
    "\n",
    "df = df.drop(*columns_to_remove)"
   ],
   "id": "d245622e4fc0f44b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The ethnicity column has too many unique values. We can group them into a few categories.",
   "id": "7776beac1e92e389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the transformation using multiple chained when conditions\n",
    "df = df.withColumn(\"ETHNICITY\",\n",
    "    F.when(F.col(\"ETHNICITY\").isin('AMERICAN INDIAN/ALASKA NATIVE', 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE'),\n",
    "           'American Indian/Alaska Native')\n",
    "    .when(F.col(\"ETHNICITY\").isin('ASIAN', 'ASIAN - ASIAN INDIAN', 'ASIAN - CAMBODIAN', 'ASIAN - CHINESE', 'ASIAN - FILIPINO', 'ASIAN - JAPANESE', 'ASIAN - KOREAN', 'ASIAN - OTHER', 'ASIAN - THAI', 'ASIAN - VIETNAMESE'),\n",
    "          'Asian')\n",
    "    .when(F.col(\"ETHNICITY\").isin('BLACK/AFRICAN', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN', 'BLACK/HAITIAN'),\n",
    "          'Black')\n",
    "    .when(F.col(\"ETHNICITY\").isin('HISPANIC OR LATINO', 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)', 'HISPANIC/LATINO - COLOMBIAN', 'HISPANIC/LATINO - CUBAN', 'HISPANIC/LATINO - DOMINICAN', 'HISPANIC/LATINO - GUATEMALAN', 'HISPANIC/LATINO - HONDURAN', 'HISPANIC/LATINO - MEXICAN', 'HISPANIC/LATINO - PUERTO RICAN', 'HISPANIC/LATINO - SALVADORAN'),\n",
    "          'Hispanic/Latino')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MIDDLE EASTERN'),\n",
    "          'Middle Eastern')\n",
    "    .when(F.col(\"ETHNICITY\").isin('NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER'),\n",
    "          'Pacific Islander')\n",
    "    .when(F.col(\"ETHNICITY\").isin('WHITE', 'WHITE - BRAZILIAN', 'WHITE - EASTERN EUROPEAN', 'WHITE - OTHER EUROPEAN', 'WHITE - RUSSIAN', 'PORTUGUESE'),\n",
    "          'White')\n",
    "    .when(F.col(\"ETHNICITY\").isin('CARIBBEAN ISLAND', 'SOUTH AMERICAN'),\n",
    "          'Caribbean/South American')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MULTI RACE ETHNICITY'),\n",
    "          'Multi-Race')\n",
    "    .when(F.col(\"ETHNICITY\").isin('OTHER'),\n",
    "          'Other')\n",
    "    .otherwise('NO DATA REGISTERED')\n",
    ")"
   ],
   "id": "dab8818e9e64a178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can aggregate the data by primary keys and collect the remaining columns into lists.",
   "id": "dd9393f9b525a73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your primary key columns\n",
    "primary_key_columns = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "# Identify the remaining columns to be grouped\n",
    "remaining_columns = [col for col in df.columns if col not in primary_key_columns]\n",
    "\n",
    "# Group by the primary key columns and aggregate the remaining columns into lists\n",
    "df = df.groupBy(primary_key_columns).agg(*(F.collect_list(col).alias(col) for col in remaining_columns))"
   ],
   "id": "426159d6db44f48a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can transform the columns that contain lists of values. If all values in the list are the same, we can replace the list with a single value. If the list is empty, we can replace it with a default value.",
   "id": "85db3f5cb0247fc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "1250530cd4aa7223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "replace_empty_list_udf = F.udf(utils.replace_empty_list, ArrayType(StringType()))\n",
    "transform_list_udf = F.udf(utils.transform_list, ArrayType(StringType()))\n",
    "handle_list_udf = F.udf(utils.handle_list, StringType())\n",
    "empty_list_udf = F.udf(lambda col: col == [], BooleanType())\n",
    "\n",
    "for column in df.columns:    \n",
    "    if isinstance(df.schema[column].dataType, ArrayType):\n",
    "        if df.filter(empty_list_udf(F.col(column))).count() > 0:\n",
    "            df = df.withColumn(column, replace_empty_list_udf(F.col(column)))\n",
    "            \n",
    "        df = df.withColumn(column, transform_list_udf(F.col(column)))\n",
    "        if len(set(df.select(column).first()[0])) == 1:\n",
    "            df = df.withColumn(column, handle_list_udf(df[column]))"
   ],
   "id": "caae3630b4ca832b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values again.",
   "id": "f32cfe22255a064f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "93db242a248ae66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "89c7ee6e59b54f6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's analyze the diseases column. We can explode the column and calculate the mean length of stay for each disease code. Then we can rank the diseases for each patient and keep the top-ranked one.",
   "id": "3959f4fa440aa15b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Explode the DISEASES_CODE column\n",
    "exploded_df = df.withColumn(\"DISEASES_CODE\", F.explode(F.col(\"DISEASES_CODE\")))\n",
    "\n",
    "# Calculate mean LOS for each disease code\n",
    "disease_mean_los = exploded_df.groupBy(\"DISEASES_CODE\").agg(F.mean(\"LOS\").alias(\"mean_LOS\"))\n",
    "\n",
    "# Join the mean LOS back to the exploded dataframe\n",
    "joined_df = exploded_df.join(disease_mean_los, on=\"DISEASES_CODE\", how=\"left\")\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"SUBJECT_ID\", \"HADM_ID\").orderBy(F.col(\"mean_LOS\").desc())\n",
    "\n",
    "# Rank disease codes for each patient and filter to keep the top-ranked one\n",
    "most_influential_disease_df = joined_df.withColumn(\"rank\", F.row_number().over(window_spec)).filter(F.col(\"rank\") == 1)\n",
    "\n",
    "# Select the relevant columns\n",
    "result_df = most_influential_disease_df.select(\"SUBJECT_ID\", \"HADM_ID\", \"DISEASES_CODE\")\n",
    "\n",
    "# Ensure DISEASES_CODE is a string in both DataFrames\n",
    "df = df.withColumn(\"DISEASES_CODE\", F.col(\"DISEASES_CODE\").cast(\"string\"))\n",
    "result_df = result_df.withColumn(\"DISEASES_CODE\", F.col(\"DISEASES_CODE\").cast(\"string\"))\n",
    "\n",
    "# Drop the DISEASES_CODE column from df before joining\n",
    "df = df.drop(\"DISEASES_CODE\")\n",
    "\n",
    "# Join df with result_df on SUBJECT_ID and HADM_ID\n",
    "updated_df = df.join(result_df, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "\n",
    "# Add the updated DISEASES_CODE column from result_df\n",
    "df = updated_df.withColumn(\n",
    "    \"DISEASES_CODE\",\n",
    "    F.coalesce(result_df[\"DISEASES_CODE\"], F.col(\"DISEASES_CODE\"))\n",
    ")\n",
    "\n",
    "df = df.withColumnRenamed(\"DISEASES_CODE\", \"DISEASE_CODE\")"
   ],
   "id": "80561f8083cc6450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the updated DataFrame\n",
    "df.show()"
   ],
   "id": "815b2be1e78dd9a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graphical Analysis",
   "id": "b44f05a9d055d4ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of marital status",
   "id": "71dc1b8d55dda66e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'MARITAL_STATUS', 'MARITAL_STATUS', F.count, 'Count of Marital Status', 'Marital Status', 'Count')",
   "id": "4ee2457f777f3541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of Admission Types",
   "id": "633859d36b485929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'ADMISSION_TYPE', 'ADMISSION_TYPE', F.count, 'Count of Admission Types', 'Admission Type', 'Count')",
   "id": "7b13b06a0668c0ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Gender",
   "id": "cc414f59bd23ced4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate average LOS per gender\n",
    "average_los_per_gender = df.groupBy('GENDER')\\\n",
    "    .agg(F.avg('LOS').alias('Average LOS'))\\\n",
    "    .orderBy(F.col('Average LOS').desc())\n",
    "\n",
    "# Call the plot_graph function\n",
    "utils.plot_graph(\n",
    "    average_los_per_gender,\n",
    "    'GENDER',\n",
    "    'Average LOS',\n",
    "    F.first,\n",
    "    'Comparison of Genders by Average Length of Stay (LOS)',\n",
    "    'Gender',\n",
    "    'Average LOS (Days)'\n",
    ")"
   ],
   "id": "af15daa20f2a9008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Age",
   "id": "f9caa8e6192f0dd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate average LOS per age\n",
    "average_los_per_age = df.filter(df['AGE'] < 100).groupBy('AGE')\\\n",
    "    .agg(F.avg('LOS').alias('Average LOS'))\\\n",
    "    .orderBy(F.col('Average LOS').desc())\\\n",
    "    .limit(10)\n",
    "\n",
    "# Call the plot_graph function\n",
    "utils.plot_graph(\n",
    "    average_los_per_age,\n",
    "    'AGE',\n",
    "    'Average LOS',\n",
    "    F.first,\n",
    "    'Top 10 Ages by Average Length of Stay (LOS)',\n",
    "    'Age',\n",
    "    'Average LOS (Days)'\n",
    ")"
   ],
   "id": "112d747db3592e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Religion",
   "id": "1a2938601acce3b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter out rows where RELIGION is 'NO DATA REGISTERED'\n",
    "filtered_df = df.filter(df['RELIGION'] != 'UNOBTAINABLE')\n",
    "\n",
    "# Calculate average LOS per religion\n",
    "average_los_per_religion = filtered_df.groupBy('RELIGION')\\\n",
    "    .agg(F.avg('LOS').alias('Average LOS'))\\\n",
    "    .orderBy(F.col('Average LOS').desc())\\\n",
    "    .limit(10)\n",
    "\n",
    "# Call the plot_graph function\n",
    "utils.plot_graph(\n",
    "    average_los_per_religion,\n",
    "    'RELIGION',\n",
    "    'Average LOS',\n",
    "    F.first,\n",
    "    'Top 10 Religions by Average Length of Stay (LOS)',\n",
    "    'Religion',\n",
    "    'Average LOS (Days)'\n",
    ")"
   ],
   "id": "248e26d8375e8038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Ethnicity",
   "id": "7152930eda22b113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter out rows where ETHNICITY is 'NO DATA REGISTERED'\n",
    "filtered_df = df.filter(df['ETHNICITY'] != 'NO DATA REGISTERED')\n",
    "\n",
    "# Calculate average LOS per ethnicity\n",
    "average_los_per_ethnicity = filtered_df.groupBy('ETHNICITY')\\\n",
    "    .agg(F.avg('LOS').alias('Average LOS'))\\\n",
    "    .orderBy(F.col('Average LOS').desc())\\\n",
    "    .limit(10)\n",
    "\n",
    "# Call the plot_graph function\n",
    "utils.plot_graph(\n",
    "    average_los_per_ethnicity,\n",
    "    'ETHNICITY',\n",
    "    'Average LOS',\n",
    "    F.first,\n",
    "    'Top 10 Ethnicities by Average Length of Stay (LOS)',\n",
    "    'Ethnicity',\n",
    "    'Average LOS (Days)'\n",
    ")"
   ],
   "id": "938682c216bf5377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "141db43ff2fee214"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre processing",
   "id": "52d7e3f536e8fcfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's convert the categorical columns to numerical columns. And let's get the categorical and numerical columns names.",
   "id": "713f6298f3254135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_columns = df.columns\n",
    "feature_columns = [col for col in all_columns if col not in ['SUBJECT_ID', 'HADM_ID', LABEL]]\n",
    "\n",
    "numerical_features = ['FIRST_WARDID', 'LAST_WARDID', 'AGE']\n",
    "\n",
    "categorical_features = [col for col in feature_columns if col not in numerical_features and col not in ['SUBJECT_ID', 'HADM_ID', LABEL]]\n",
    "\n",
    "# Cast string numerical features to float\n",
    "df = df.withColumn(\"AGE\", F.col(\"AGE\").cast(\"float\"))\n",
    "df = df.withColumn(\"FIRST_WARDID\", F.col(\"FIRST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LAST_WARDID\", F.col(\"LAST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LOS\", F.col(\"LOS\").cast(\"float\"))\n",
    "\n",
    "print(f\"Numerical Features: {numerical_features}\\n\")\n",
    "print(f\"Categorical Features: {categorical_features}\\n\")"
   ],
   "id": "dde019ceeaa3b584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stages in the pipeline\n",
    "stages = []\n",
    "\n",
    "# Indexing and encoding categorical features\n",
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"ClassVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assemble all the features along with the encoded categorical features\n",
    "assemblerInputs = [c + \"ClassVec\" for c in categorical_features] + numerical_features\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Pipeline: This will ensure all stages are applied in sequence\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_transformed = pipelineModel.transform(df)"
   ],
   "id": "4fc81fbf35516275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we are making sure that there isn't simultaneously a patient in the test set and train set.",
   "id": "dc2264ddb657c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume subject_ids have been collected as before\n",
    "subject_ids = [row['SUBJECT_ID'] for row in df_transformed.select(\"SUBJECT_ID\").distinct().collect()]\n",
    "split_index = int(len(subject_ids) * 0.8)\n",
    "\n",
    "train_ids = set(subject_ids[:split_index])\n",
    "test_ids = set(subject_ids[split_index:])\n",
    "\n",
    "# Directly filter the DataFrame using the list\n",
    "train_df = df_transformed.filter(F.col(\"SUBJECT_ID\").isin(train_ids))\n",
    "test_df = df_transformed.filter(F.col(\"SUBJECT_ID\").isin(test_ids))\n",
    "\n",
    "# Ensure that both train and test data have non-null labels and drop the subject_id and hadm_id columns because they are not needed\n",
    "train_df = train_df.filter(train_df[LABEL].isNotNull()).drop(\"SUBJECT_ID\").drop(\"HADM_ID\")\n",
    "test_df = test_df.filter(test_df[LABEL].isNotNull()).drop(\"SUBJECT_ID\").drop(\"HADM_ID\")"
   ],
   "id": "d2ef21a5b75c8ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction",
   "id": "8f4a8adc7d56a64d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression",
   "id": "ea9c5b48e71b17cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tuned_param = {\n",
    "    'maxIter': 10,\n",
    "    'regParam': 0.3,\n",
    "    'elasticNetParam': 0.8\n",
    "}\n",
    "\n",
    "# Define and fit the Linear Regression model on the training set\n",
    "lr = LinearRegression(featuresCol='features', labelCol=LABEL, **tuned_param)\n",
    "lr_model = lr.fit(train_df)\n",
    "pred_df = lr_model.transform(test_df)"
   ],
   "id": "ff9b5170b8210b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_metrics(RegressionEvaluator(labelCol=\"LOS\", predictionCol=\"prediction\"), pred_df)",
   "id": "8552e500294cc4d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Random Forest Regression"
   ],
   "id": "d32a48b1e9035b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tuned_param = {\n",
    "    'maxDepth': 5,\n",
    "    'maxBins': 32,\n",
    "    'minInstancesPerNode': 1,\n",
    "    'minInfoGain': 0.0,\n",
    "    'maxMemoryInMB': 256,\n",
    "    'cacheNodeIds': False,\n",
    "    'checkpointInterval': 10,\n",
    "    'impurity': 'variance',\n",
    "    'featureSubsetStrategy': 'auto',\n",
    "    'subsamplingRate': 1.0,\n",
    "    'seed': None,\n",
    "    'numTrees': 20,\n",
    "}\n",
    "\n",
    "# Define and fit the Random Forest model on the training set\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol=LABEL, **tuned_param)\n",
    "rf_model = rf.fit(train_df)\n",
    "pred_df = rf_model.transform(test_df)"
   ],
   "id": "31943b611838f331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_metrics(RegressionEvaluator(labelCol=\"LOS\", predictionCol=\"prediction\"), pred_df)",
   "id": "5f85e1b360df97f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
