{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import utils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, count\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType, IntegerType, DoubleType\n",
    "\n",
    "LABEL = 'LOS'"
   ],
   "id": "500926c2d111d3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Intensive Care Unit Data Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# check spark configs to only errors:\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"1000\")"
   ],
   "id": "16b2dd3593ae37fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the datasets\n",
    "df_admissions = spark.read.csv(\"datasets/ADMISSIONS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_diagnoses = spark.read.csv(\"datasets/DIAGNOSES_ICD.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_icustays = spark.read.csv(\"datasets/ICUSTAYS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "df_patients = spark.read.csv(\"datasets/PATIENTS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")\n",
    "# df_chartevents = spark.read.csv(\"datasets/CHARTEVENTS.csv\", header=True, inferSchema=True).drop(\"ROW_ID\")"
   ],
   "id": "f10eb3861e1a6ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_patients.join(df_admissions, df_patients[\"SUBJECT_ID\"] == df_admissions[\"SUBJECT_ID\"], how=\"left\").drop(df_admissions[\"SUBJECT_ID\"])\n",
    "df = df.join(df_icustays, df[\"HADM_ID\"] == df_icustays[\"HADM_ID\"], how=\"left\").drop(df_icustays[\"SUBJECT_ID\"]).drop(df_icustays[\"HADM_ID\"])\n",
    "df = df.join(df_diagnoses, df[\"SUBJECT_ID\"] == df_diagnoses[\"SUBJECT_ID\"], how=\"left\").drop(df_diagnoses[\"SUBJECT_ID\"]).drop(df_diagnoses[\"HADM_ID\"])"
   ],
   "id": "de5ba1599a4770a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "6f4a65a67efe1407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values in the dataset.",
   "id": "74cdea29e7496f79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "13dcfe5ad8d78111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "id": "10987a3efe7f09bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's create a column called 'AGE'",
   "id": "b13d9d8304f7de2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure 'DOB' and 'ADMITTIME' are in the correct date format if not already\n",
    "df = df.withColumn('DOB', F.to_date('DOB'))\n",
    "df = df.withColumn('ADMITTIME', F.to_date('ADMITTIME'))\n",
    "\n",
    "# Create the 'AGE' column by calculating the difference in years between 'ADMITTIME' and 'DOB'\n",
    "df = df.withColumn('AGE', F.expr(\"floor(months_between(ADMITTIME, DOB) / 12)\"))"
   ],
   "id": "def300029631d207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It only matters if the patient is dead or not, not the date of death, so we can drop the date of death columns. And we also drop other death-related columns.",
   "id": "9823fde2d264c156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df=df.drop(\"DOB\").drop(\"DOD\").drop(\"DOD_SSN\").drop(\"EXPIRE_FLAG\").drop(\"DEATHTIME\")\n",
    "df = df.withColumn(\"DOD_HOSP\", F.when(F.col(\"DOD_HOSP\").isNull(), 0).otherwise(1))\n",
    "df = df.withColumnRenamed(\"DOD_HOSP\", \"DIED\")\n",
    "df = df.withColumnRenamed(\"ICD9_CODE\",\"DISEASES_CODE\")"
   ],
   "id": "2bde8175746ee315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we will drop the columns that are not useful for the analysis.",
   "id": "41180090f1dbf0a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_remove = [\n",
    "    \"ADMITTIME\", \"DISCHTIME\", \"EDREGTIME\", \"EDOUTTIME\", \"HOSPITAL_EXPIRE_FLAG\",\n",
    "    \"INTIME\", \"OUTTIME\",\"LANGUAGE\",\"DISCHARGE_LOCATION\",\n",
    "    \"ICUSTAY_ID\", \"SEQ_NUM\",\"HAS_CHARTEVENTS_DATA\",\"DBSOURCE\"\n",
    "]\n",
    "\n",
    "df = df.drop(*columns_to_remove)"
   ],
   "id": "d245622e4fc0f44b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The ethnicity column has too many unique values. We can group them into a few categories.",
   "id": "7776beac1e92e389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the transformation using multiple chained when conditions\n",
    "df = df.withColumn(\"ETHNICITY\",\n",
    "    F.when(F.col(\"ETHNICITY\").isin('AMERICAN INDIAN/ALASKA NATIVE', 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE'),\n",
    "           'American Indian/Alaska Native')\n",
    "    .when(F.col(\"ETHNICITY\").isin('ASIAN', 'ASIAN - ASIAN INDIAN', 'ASIAN - CAMBODIAN', 'ASIAN - CHINESE', 'ASIAN - FILIPINO', 'ASIAN - JAPANESE', 'ASIAN - KOREAN', 'ASIAN - OTHER', 'ASIAN - THAI', 'ASIAN - VIETNAMESE'),\n",
    "          'Asian')\n",
    "    .when(F.col(\"ETHNICITY\").isin('BLACK/AFRICAN', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN', 'BLACK/HAITIAN'),\n",
    "          'Black')\n",
    "    .when(F.col(\"ETHNICITY\").isin('HISPANIC OR LATINO', 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)', 'HISPANIC/LATINO - COLOMBIAN', 'HISPANIC/LATINO - CUBAN', 'HISPANIC/LATINO - DOMINICAN', 'HISPANIC/LATINO - GUATEMALAN', 'HISPANIC/LATINO - HONDURAN', 'HISPANIC/LATINO - MEXICAN', 'HISPANIC/LATINO - PUERTO RICAN', 'HISPANIC/LATINO - SALVADORAN'),\n",
    "          'Hispanic/Latino')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MIDDLE EASTERN'),\n",
    "          'Middle Eastern')\n",
    "    .when(F.col(\"ETHNICITY\").isin('NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER'),\n",
    "          'Pacific Islander')\n",
    "    .when(F.col(\"ETHNICITY\").isin('WHITE', 'WHITE - BRAZILIAN', 'WHITE - EASTERN EUROPEAN', 'WHITE - OTHER EUROPEAN', 'WHITE - RUSSIAN', 'PORTUGUESE'),\n",
    "          'White')\n",
    "    .when(F.col(\"ETHNICITY\").isin('CARIBBEAN ISLAND', 'SOUTH AMERICAN'),\n",
    "          'Caribbean/South American')\n",
    "    .when(F.col(\"ETHNICITY\").isin('MULTI RACE ETHNICITY'),\n",
    "          'Multi-Race')\n",
    "    .when(F.col(\"ETHNICITY\").isin('OTHER'),\n",
    "          'Other')\n",
    "    .otherwise('NO DATA REGISTERED')\n",
    ")"
   ],
   "id": "dab8818e9e64a178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can aggregate the data by primary keys and collect the remaining columns into lists.",
   "id": "dd9393f9b525a73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your primary key columns\n",
    "primary_key_columns = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "# Identify the remaining columns to be grouped\n",
    "remaining_columns = [col for col in df.columns if col not in primary_key_columns]\n",
    "\n",
    "# Group by the primary key columns and aggregate the remaining columns into lists\n",
    "df = df.groupBy(primary_key_columns).agg(*(F.collect_list(col).alias(col) for col in remaining_columns))\n",
    "\n"
   ],
   "id": "426159d6db44f48a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can transform the columns that contain lists of values. If all values in the list are the same, we can replace the list with a single value. If the list is empty, we can replace it with a default value.",
   "id": "85db3f5cb0247fc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "1250530cd4aa7223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "replace_empty_list_udf = udf(utils.replace_empty_list, ArrayType(StringType()))\n",
    "transform_list_udf = udf(utils.transform_list, ArrayType(StringType()))\n",
    "handle_list_udf = udf(utils.handle_list, StringType())\n",
    "empty_list_udf = F.udf(lambda col: col == [], BooleanType())\n",
    "\n",
    "for column in df.columns:    \n",
    "    if isinstance(df.schema[column].dataType, ArrayType):\n",
    "        if df.filter(empty_list_udf(F.col(column))).count() > 0:\n",
    "            df = df.withColumn(column, replace_empty_list_udf(col(column)))\n",
    "            \n",
    "        df = df.withColumn(column, transform_list_udf(col(column)))\n",
    "        if len(set(df.select(column).first()[0])) == 1:\n",
    "            df = df.withColumn(column, handle_list_udf(df[column]))"
   ],
   "id": "caae3630b4ca832b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values again.",
   "id": "f32cfe22255a064f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.print_missing_value_counts(df)",
   "id": "93db242a248ae66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show()",
   "id": "89c7ee6e59b54f6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's analyze the diseases column. We can explode the column and calculate the mean length of stay for each disease code. Then we can rank the diseases for each patient and keep the top-ranked one.",
   "id": "3959f4fa440aa15b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Explode the DISEASES_CODE column\n",
    "exploded_df = df.withColumn(\"DISEASES_CODE\", F.explode(col(\"DISEASES_CODE\")))\n",
    "\n",
    "# Calculate mean LOS for each disease code\n",
    "disease_mean_los = exploded_df.groupBy(\"DISEASES_CODE\").agg(F.mean(\"LOS\").alias(\"mean_LOS\"))\n",
    "\n",
    "# Join the mean LOS back to the exploded dataframe\n",
    "joined_df = exploded_df.join(disease_mean_los, on=\"DISEASES_CODE\", how=\"left\")\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"SUBJECT_ID\", \"HADM_ID\").orderBy(col(\"mean_LOS\").desc())\n",
    "\n",
    "# Rank disease codes for each patient and filter to keep the top-ranked one\n",
    "most_influential_disease_df = joined_df.withColumn(\"rank\", F.row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "# Select the relevant columns\n",
    "result_df = most_influential_disease_df.select(\"SUBJECT_ID\", \"HADM_ID\", \"DISEASES_CODE\")\n",
    "\n",
    "# Ensure DISEASES_CODE is a string in both DataFrames\n",
    "df = df.withColumn(\"DISEASES_CODE\", col(\"DISEASES_CODE\").cast(\"string\"))\n",
    "result_df = result_df.withColumn(\"DISEASES_CODE\", col(\"DISEASES_CODE\").cast(\"string\"))\n",
    "\n",
    "# Drop the DISEASES_CODE column from df before joining\n",
    "df = df.drop(\"DISEASES_CODE\")\n",
    "\n",
    "# Join df with result_df on SUBJECT_ID and HADM_ID\n",
    "updated_df = df.join(result_df, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "\n",
    "# Add the updated DISEASES_CODE column from result_df\n",
    "df = updated_df.withColumn(\n",
    "    \"DISEASES_CODE\",\n",
    "    F.coalesce(result_df[\"DISEASES_CODE\"], col(\"DISEASES_CODE\"))\n",
    ")\n",
    "\n",
    "df = df.withColumnRenamed(\"DISEASES_CODE\", \"MOST_IMPORTANT_DISEASE_CODE\")"
   ],
   "id": "80561f8083cc6450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the updated DataFrame\n",
    "df.show()"
   ],
   "id": "815b2be1e78dd9a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graphical Analysis",
   "id": "b44f05a9d055d4ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming `df` is your PySpark DataFrame\n",
    "df = df.withColumn(\"DIED\", df[\"DIED\"].cast(\"integer\"))\n",
    "\n",
    "# Plotting death counts by gender\n",
    "utils.plot_graph(df, 'GENDER', 'DIED', F.sum, 'Death Counts by Gender', 'Gender', 'Number of Deaths')"
   ],
   "id": "af15daa20f2a9008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of marital status",
   "id": "71dc1b8d55dda66e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'MARITAL_STATUS', 'MARITAL_STATUS', F.count, 'Count of Marital Status', 'Marital Status', 'Count')",
   "id": "4ee2457f777f3541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unique Value Count of Admission Types",
   "id": "633859d36b485929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.plot_graph(df, 'ADMISSION_TYPE', 'ADMISSION_TYPE', F.count, 'Count of Admission Types', 'Admission Type', 'Count')",
   "id": "7b13b06a0668c0ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Distribution of Lethality by Religion\n",
    "\n",
    "The 1ยบ and 2ยบ deadliest religions are not shown in the graph because they are outliers."
   ],
   "id": "1a2938601acce3b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate death rate per religion\n",
    "death_rate_per_religion = df.groupBy('RELIGION')\\\n",
    "    .agg(\n",
    "        F.sum('DIED').alias('Deaths'), \n",
    "        F.count('DIED').alias('Total')\n",
    "    )\\\n",
    "    .withColumn('Death Rate', (F.col('Deaths') / F.col('Total')) * 100)\\\n",
    "    .orderBy(F.col('Death Rate').desc())\n",
    "\n",
    "religions = death_rate_per_religion.rdd.zipWithIndex().filter(lambda x: x[1] in [2,3,4,5,6,7]).map(lambda x: x[0]).toDF()\n",
    "\n",
    "utils.plot_graph(religions, 'RELIGION', 'Death Rate', F.first, 'Death Rates for Religions Ranked', 'Religion', 'Death Rate (%)')"
   ],
   "id": "248e26d8375e8038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Lethality by Ethnicity",
   "id": "7152930eda22b113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_df = df.filter(df['ETHNICITY'] != 'NO DATA REGISTERED')\n",
    "\n",
    "# Calculate death rate per ethnicity\n",
    "death_rate_per_ethnicity = filtered_df.groupBy('ETHNICITY')\\\n",
    "    .agg(\n",
    "        F.sum('DIED').alias('Deaths'), \n",
    "        F.count('DIED').alias('Total')\n",
    "    )\\\n",
    "    .withColumn('Death Rate', (F.col('Deaths') / F.col('Total')) * 100)\\\n",
    "    .orderBy(F.col('Death Rate').desc())\\\n",
    "    .limit(5)\n",
    "\n",
    "utils.plot_graph(death_rate_per_ethnicity, 'ETHNICITY', 'Death Rate', F.first, 'Top 5 Deadliest Ethnicities by Death Rate', 'Ethnicity', 'Death Rate (%)')"
   ],
   "id": "938682c216bf5377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of Length of Stay by Age",
   "id": "bfcc45416965fed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be777315dd7a0653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre processing",
   "id": "52d7e3f536e8fcfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's convert the categorical columns to numerical columns. And let's get the categorical and numerical columns names.",
   "id": "713f6298f3254135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_columns = df.columns\n",
    "feature_columns = [col for col in all_columns if col not in ['SUBJECT_ID', LABEL]]\n",
    "\n",
    "# Manually specified categorical and numerical features\n",
    "categorical_features = ['ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE', 'RELIGION',\n",
    "                        'MARITAL_STATUS', 'ETHNICITY', 'DIAGNOSIS', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'MOST_IMPORTANT_DISEASE_CODE']\n",
    "numerical_features = [col for col in feature_columns if col not in categorical_features and col not in ['SUBJECT_ID', LABEL]]\n",
    "\n",
    "# Transform the 'M' and 'F' values to 0 and 1 respectively\n",
    "df = df.withColumn(\"GENDER\", F.when(col(\"GENDER\") == \"M\", 0).otherwise(1))\n",
    "# Cast string numerical features to float\n",
    "df = df.withColumn(\"AGE\", col(\"AGE\").cast(\"float\"))\n",
    "df = df.withColumn(\"FIRST_WARDID\", col(\"FIRST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LAST_WARDID\", col(\"LAST_WARDID\").cast(\"float\"))\n",
    "df = df.withColumn(\"LOS\", col(\"LOS\").cast(\"float\"))\n",
    "\n",
    "df.show()"
   ],
   "id": "dde019ceeaa3b584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stages in the pipeline\n",
    "stages = []\n",
    "\n",
    "# Indexing and encoding categorical features\n",
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"ClassVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assemble all the features along with the encoded categorical features\n",
    "assemblerInputs = [c + \"ClassVec\" for c in categorical_features] + numerical_features\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Pipeline: This will ensure all stages are applied in sequence\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_transformed = pipelineModel.transform(df)"
   ],
   "id": "4fc81fbf35516275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we are making sure that there isn't simultaneously a patient in the test set and train set.",
   "id": "dc2264ddb657c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume subject_ids have been collected as before\n",
    "subject_ids = [row['SUBJECT_ID'] for row in df_transformed.select(\"SUBJECT_ID\").distinct().collect()]\n",
    "split_index = int(len(subject_ids) * 0.8)\n",
    "\n",
    "train_ids = set(subject_ids[:split_index])\n",
    "test_ids = set(subject_ids[split_index:])\n",
    "\n",
    "# Directly filter the DataFrame using the list\n",
    "train_df = df_transformed.filter(F.col(\"SUBJECT_ID\").isin(train_ids))\n",
    "test_df = df_transformed.filter(F.col(\"SUBJECT_ID\").isin(test_ids))\n",
    "\n",
    "# Ensure that both train and test data have non-null labels\n",
    "train_df = train_df.filter(train_df[LABEL].isNotNull())\n",
    "test_df = test_df.filter(test_df[LABEL].isNotNull())\n",
    "\n",
    "# Split the test_df into features and labels\n",
    "X_test = test_df.drop(LABEL)\n",
    "y_test = test_df.select(LABEL)"
   ],
   "id": "d2ef21a5b75c8ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction",
   "id": "8f4a8adc7d56a64d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression",
   "id": "ea9c5b48e71b17cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param = {\n",
    "    'maxIter': 10,\n",
    "    'regParam': 0.3,\n",
    "    'elasticNetParam': 0.8\n",
    "}\n",
    "\n",
    "# Define and fit the Linear Regression model on the training set\n",
    "lr = LinearRegression(featuresCol='features', labelCol=LABEL, **param)\n",
    "lr_model = lr.fit(train_df)"
   ],
   "id": "ff9b5170b8210b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = lr_model.transform(X_test)\n",
    "print(f\"Linear Regression - Mean Residual: {utils.mean_residuals(y_pred,y_test)}\")"
   ],
   "id": "90dd9305c54d5891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest Regression",
   "id": "d32a48b1e9035b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param = {\n",
    "    'maxDepth': 5,\n",
    "    'maxBins': 32,\n",
    "    'minInstancesPerNode': 1,\n",
    "    'minInfoGain': 0.0,\n",
    "    'maxMemoryInMB': 256,\n",
    "    'cacheNodeIds': False,\n",
    "    'checkpointInterval': 10,\n",
    "    'impurity': 'variance',\n",
    "    'featureSubsetStrategy': 'auto',\n",
    "    'subsamplingRate': 1.0,\n",
    "    'seed': None,\n",
    "    'numTrees': 20,\n",
    "}\n",
    "\n",
    "# Define and fit the Random Forest model on the training set\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol=LABEL, **param)\n",
    "rf_model = rf.fit(train_df)"
   ],
   "id": "31943b611838f331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = rf_model.transform(X_test)\n",
    "print(f\"Random Forest - Mean Residual: {utils.mean_residuals(y_pred,y_test)}\")"
   ],
   "id": "5f85e1b360df97f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
